{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlWvXrofgLmFtwSTlU9cS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AamirJafaq/LinearRegression/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "WfGZrAOAs9e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is a statistical technique used to find the relationship between variables. In an machine learning context, linear regression finds the relationship between features and a label."
      ],
      "metadata": {
        "id": "Pm1febkZtHgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equation for a linear regression model can be written as follows:\n",
        "$$y=xw+b$$\n",
        "where $y$ is predicted vector, $w$ is weight vector, $x$ is feature matrix and $b$ is bias. During training, the model calculates the weight and bias that produce the best model."
      ],
      "metadata": {
        "id": "g7NGDwhWtRSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "u-zO37so-o8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function is a numerical measure of how far a model’s predictions deviate from the actual values. It quantifies the error by comparing predicted outputs with the true labels. The objective of training a model is to minimize this loss, bringing it as close to zero as possible. In linear regression, four primary types of loss functions are commonly used, which are described below:"
      ],
      "metadata": {
        "id": "3MMmkTQm_DTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Mean Square Error:**\n",
        "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "where $y_i$ is actual value and $\\hat{y}_i$ is predicted value.\\\n",
        "**Mean Absolute Error:**\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$"
      ],
      "metadata": {
        "id": "oPpiHBnpAQ3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the right loss function in linear regression depends on the nature of the data and how outliers should be handled. For example, with MSE, the model fits closer to outliers but may deviate more from the majority of data points. In contrast, with MAE, the model stays closer to most data points while being less influenced by outliers."
      ],
      "metadata": {
        "id": "g3Ki8QEdCiAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent"
      ],
      "metadata": {
        "id": "1D0BJgoCEMf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is a core optimization algorithm in machine learning and deep learning that iteratively updates a model’s parameters (such as weights and biases) to minimize a loss or cost function. It works by moving in the direction of the steepest decrease (negative gradient), with the learning rate controlling the step size, until the function reaches its minimum."
      ],
      "metadata": {
        "id": "Vke1SLx_ERnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate (also referred to as step size or the alpha) is the size of the steps that are taken to reach the minimum. High learning rates result in larger steps but risks overshooting the minimum. Conversely, a low learning rate has small step sizes. While it has the advantage of more precision, the number of iterations compromises overall efficiency as this takes more time and computations to reach the minimum. Therefore, choose a learning rate that is balanced not too high and not too low so the model converges quickly and smoothly."
      ],
      "metadata": {
        "id": "R784x8YpGOGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent and mini-batch gradient descent. \\\n",
        "**Batch gradient descent**"
      ],
      "metadata": {
        "id": "Z3LS7VeZG4bO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAAkN9HZH5Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}